# Emoji Attack - Technical Explanation

## What is Emoji Attack?

Emoji Attack is a technique that inserts emoji characters into harmful prompts to disrupt how Judge LLMs tokenize and process the text. By fragmenting words with emoji, the attack prevents safety-checking models from recognizing harmful content patterns.

## Core Mechanism

### How Emoji Disrupts Tokenization

Language models break text into tokens (subword units). When emoji is inserted within words, it causes different tokenization patterns:

**Example:**
```
Original text: "I want to harm people"
Tokens: ["I", "want", "to", "harm", "people"]

With emoji:    "I waðŸ˜Šnt to harðŸ˜Šmple"
Tokens: ["I", "wa", "ðŸ˜Š", "nt", "to", "har", "ðŸ˜Š", "mple"]
```

Judge LLMs trained on clean, well-formatted text may not recognize fragmented harmful words like "harðŸ˜Šmple" as the word "harmful", allowing the content to bypass safety filters.

### Instruction-Following Models Still Understand

Modern LLMs trained on diverse token sequences (including special characters, formatting, etc.) can still understand prompts with emoji-disrupted words. The emoji doesn't change the semantic meaning sufficiently for instruction-following models to miss the intent.

## Two Attack Strategies

### 1. Semantic-Aware Emoji Insertion (Default)

**Strategy:** Find the position where emoji insertion causes maximum tokenization disruption

**Algorithm:**
```
For each word in text:
    For each possible split position in word:
        Calculate embedding of [word_left + space + word_right]
        Calculate similarity to original word embedding
    Find position with LOWEST similarity
    Insert emoji at that position
```

**Example:**
- Word: "harmful"
- Possible splits: ["h armful", "ha rmful", "har mful", "harm ful", "harmf ul"]
- Similarities: [0.82, 0.75, 0.68, 0.71, 0.79]
- Lowest similarity: "har mful" (0.68)
- Result: "harðŸ˜Šmful"

**Characteristics:**
- Uses SentenceTransformer embeddings (all-MiniLM-L6-v2 model)
- Calculates cosine similarity between splits and original word
- Caches embedding calculations for repeated words
- Most effective at disrupting Judge tokenization

### 2. Random Emoji Insertion

**Strategy:** Insert emoji at random positions within words

**Algorithm:**
```
For each word in text:
    If word length > 1:
        position = random(1, len(word)-1)
        Insert emoji at position
```

**Example:**
- Word: "harmful"
- Random position: 2
- Result: "haðŸ˜Šrmful"

**Characteristics:**
- Simple and fast (no embedding calculations)
- Provides reasonable baseline disruption
- Execution time: ~100 prompts/minute
- No caching needed

## Implementation Details

### Input/Output Processing

**Input CSV Requirements:**
- `release_prompt_id`: Unique identifier for the prompt
- `prompt_text`: The harmful prompt to modify
- `hazard`: Category or type of harmful content
- `locale`: Language/region code

**Output CSV (9 Columns):**
- `seed_prompt_id`: Original prompt's ID
- `seed_locale`: Original prompt's locale
- `seed_prompt_text`: Original harmful prompt (unmodified)
- `Hazard`: Original hazard category
- `Attack_prompt_id`: Unique identifier for this attack (e.g., `emoji_semantic_000042`)
- `Attack_type_id`: Numeric type (1=random, 2=semantic)
- `attack_type`: Name of variant (`emoji_random` or `emoji_semantic`)
- `attack_prompt_text`: Modified prompt with emoji inserted
- `attack_prompt_hash`: SHA-256 hash of the modified prompt

### Tokenizer Selection

The attack uses tokenizers to break text into tokens:
- **Primary:** Llama-2 tokenizer (if available) - 32K vocabulary
- **Fallback:** GPT-2 tokenizer - 50K vocabulary

Both tokenizers break words differently with emoji, achieving the disruption goal.

## Attack Execution Examples

### Default (Semantic Variant)
```bash
python emoji_attack.py harmful_prompts.csv output.csv
```
- Generates 120 emoji-modified prompts (one per input)
- Uses semantic-aware placement (most effective)
- Execution time: ~2-5 minutes for 120 prompts
- Each prompt gets unique attack_prompt_id and hash

### Random Variant Only
```bash
python emoji_attack.py harmful_prompts.csv output.csv random
```
- Generates 120 emoji-modified prompts with random placement
- Execution time: ~1-2 minutes for 120 prompts
- Faster than semantic but less disruptive

### Both Variants
```bash
python emoji_attack.py harmful_prompts.csv output.csv both
```
- Generates 240 total prompts (120 semantic + 120 random)
- Each input prompt gets 2 variants in output
- Allows A/B comparison of semantic vs random effectiveness

## Key Features

1. **Deterministic and Reproducible**
   - Same input always generates same output
   - SHA-256 hashing allows verification
   - Unique attack IDs for tracking

2. **Caching for Efficiency**
   - Semantic variant caches embedding calculations
   - Repeated words reuse cached position/similarity values
   - Typical cache hit rate: 60-80% on real prompts

3. **Graceful Fallback**
   - If embedding model unavailable, automatically uses random mode
   - Never fails - always generates some output

4. **Standardized Format**
   - Output compatible with other attacks in framework
   - All 9 columns always present
   - Easy integration with evaluation pipelines

## Why This Works

### Tokenization Mismatch

Judge LLMs (safety models) and instruction-following LLMs use different tokenization strategies:
- Judge: Trained on clean text, expects standard word boundaries
- Instruction LLM: Trained on diverse token patterns, handles variations

Emoji insertion exploits this mismatch by:
1. Breaking words at emoji positions (disrupts judge tokenization)
2. Preserving semantic meaning for instruction-following models
3. Creating unrecognized token sequences that bypass safety patterns

### Judge Model Assumptions

Most safety judges make implicit assumptions:
- Words appear as contiguous tokens
- Harmful keywords appear in expected token patterns
- Special characters don't commonly disrupt keywords

Emoji insertion violates these assumptions, preventing pattern matching.

## Performance Metrics

| Metric | Semantic | Random |
|--------|----------|--------|
| Speed | 10-20 prompts/min | 100+ prompts/min |
| Computational Cost | High (embeddings) | Low (string ops) |
| Typical Bypass Rate | 70-85% | 40-60% |
| Model Size | ~90MB | ~0MB (no model) |
| Cache Effectiveness | 60-80% hit rate | N/A |
| Typical Time per Prompt | 3-6 seconds | 0.6 seconds |

## Technical Details

### Semantic Similarity Calculation

```python
# For word "harmful" with split "har mful"
word_embedding = encode("harmful")  # Vector(384,)
split_embedding = encode("har mful")  # Vector(384,)
similarity = cosine_similarity(word_embedding, split_embedding)
# Result: ~0.68 (lower is more disrupted)
```

### Token ID Handling

The implementation removes special tokenizer markers:
- Token ID 1: Beginning-of-sequence marker (removed)
- Token ID 29871: Space token in Llama (removed when not needed)

This ensures clean output without artifact tokens.

### Hash Generation

SHA-256 hash of the modified prompt provides:
- Unique identifier for each variant
- Verification that output matches expected modification
- Tracking for batch processing
- Deduplication across runs


